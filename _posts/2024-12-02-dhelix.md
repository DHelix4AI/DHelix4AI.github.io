---
layout: post
title: "Hiding Communication Cost in Distributed LLM Training via Micro-batch Co-execution"
date:   2024-12-02
tags: ['Large Language Model','Training','Parallel']
comments: true
author: haiqwa
---

Recent advances in Generative AI, especially in chatbots and text generation, have fueled the rise of LLM training. However, communication overhead from intra-layer parallelism remains a major bottleneck. We introduce DHelix, a micro-structure that boosts training efficiency using Strand Interleaving (SI) to co-schedule forward and backward passes of micro-batches. Compatible with existing parallelism strategies, DHelix improves training throughput by up to 1.4× over Megatron-LM on various GPU clusters, effectively hiding communication overhead. Learn more in our paper: [arXiv](https://arxiv.org/pdf/2411.15871).

# End-to-end Performance

We evaluated DHelix's training throughput against Megatron-LM, Intra-batch, and Wavelet+ on 64\*A40 (48GB), 64\*A800 (80GB), and 32\*H100 (80GB) clusters, using GPT, Llama, and Phi3.5 (an MoE model). To fit GPU memory limits, model layers were adjusted, e.g., Llama-25B from Llama3.1-70B.  
DHelix achieved 12-40% (up to 58% MFU) and 2-29% (up to 71% MFU) improvements on A40 and A800 clusters, respectively, outperforming state-of-the-art methods.  
<!-- <div align="center">  -->

![figure-1](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-1.png)
![figure-2](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-2.png)
<!-- </div> -->
On the H100 cluster, though the faster network reduces DHelix’s profit margin, it makes cross-node tensor parallelism promising, a practice currently prohibitive due to communication costs. 
<div align="center"> 

![figure-3](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-3.png)
</div>


# Existing Problems

In distributed training of large language models (LLMs), besides data parallelism (DP), multi-dimensional parallel strategies are essential to address the rapid growth in model size and resource demands. These include pipeline parallelism (PP), tensor parallelism (TP), sequence parallelism (SP), context parallelism (CP), and expert parallelism (EP). Among these, the combination of TP and SP (referred to as TP+SP) is widely used in the industry. Collectively, TP+SP, CP, and EP are known as intra-layer parallelism. The figure below summarizes the data partitioning and communication processes for these three types of intra-layer parallelism:  
<div align="center"> 

![figure-4](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-4.png)
</div>
* **TP+SP**: Split a single model layer across devices, using AllGather and ReduceScatter for intermediate result exchange.  
* **CP**: Partition long sequences across devices, with AllGather and ReduceScatter handling sequence assembly and distribution. Some implementations replace collective communication with Send/Recv.  
* **EP**: For sparse models (e.g., MoE), assign experts to devices and route tokens using All-to-All communication.

Intra-layer parallel strategies introduce substantial communication overhead, which is critical to computation. For example, in the Llama-39B model, TP and CP communication account for 55% of total execution time, while the Phi-31B model sees 34.3% overhead under expert parallelism.  

Although some optimization techniques pipeline communication and computation by splitting them into smaller chunks, the sequential dependencies in transformer models greatly limit the communication that can be hidden within a single batch.
<div align="center"> 

![figure-5](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-5.png)
</div>

Therefore, we propose DHelix, which improves training efficiency by coupling forward and backward  on different micro-batches to hide the communication overhead generated in TP+SP, CP, and EP as much as possible.

# The Secret Recipe: Strand Interleaving

## Overview
<div align="center"> 

![figure-6](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-6.png)
</div>
We found that overlapping two micro-batches provides opportunities to hide intra-layer communication. Each iteration splits into independent micro-batches, alternating forward and backward passes on GPUs. These are grouped into α- and β-strands, interleaved on a single GPU. To balance memory usage, a time offset aligns their forward and backward passes. Operators are paired, such as matching communication with computation, to hide overhead. However, strand interleaving faces challenges:  
1\. Compatibility with pipeline parallelism: In the default 1F1B pipeline schedule (Figure a), opportunities for overlapping the forward and backward passes of two V-shaped micro-batches are limited.    
2\. Alternative implementation: A bidirectional pipeline (Figure b) could address this, but it requires additional model parameter replicas.    
3\. Optimal operator pairing: Determining the best strategy for pairing operators between the two strands to maximize the benefits of interleaving.  
<div align="center"> 

![figure-7](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-7.png)
</div>
Next, let's explore how to tackle the challenges mentioned above.

## Model Folding
<div align="center"> 

![figure-8](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-8.png)
</div>
Unlike the default 1F1B strategy, which maps model layers sequentially, we fold the model into a U-shape. For example, in a 32-layer model, layers 0-3 and 28-31 are placed on G0, allowing forward computation on the β-strand and backward computation on the α-strand to start from G0 and flow in the same direction, without replicating parameters. However, this folding causes each micro-batch to form a W-shape, traversing the pipeline twice.  
As a result, communication between adjacent stages doubles, but the overhead is minimal and has negligible impact on performance.

## Strand Pairing with Operator Coupling 
<div align="center"> 

![figure-9](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-9.png)
</div>

We now focus on the coupled forward+backward pass from the two strands. While U-shaped model folding enables co-execution, operator-level overlap drives DHelix's main performance gains.  
As shown in the figure, the workflow includes: (1) generating all possible operator sequences from the DAG, (2) partitioning forward/backward sequences into segments, and (3) using dynamic programming to find the optimal SI pairing, implemented by inserting barriers during co-execution.

# Conclusion
<div align="center"> 

![figure-10](https://raw.githubusercontent.com/haiqwa/haiqwa.github.io/master/images/dhelix/figure-10.png)
</div>

We propose DHelix, which leverages strand interleaving to treat micro-batches as interleaving strands and adopts a U-shaped model with an efficient scheduling strategy for forward-backward pairing. As shown in the figure, similar to DNA's double strands, two micro-batches are bound together, alternating between GPUs. By optimizing pairing strategies, DHelix effectively reduces communication overhead and improves resource utilization, demonstrating its potential for large-scale model training.  









